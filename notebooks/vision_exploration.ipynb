{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer Exploration\n",
    "\n",
    "This notebook provides interactive exploration of vision transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Visualize an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample image\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/image_classification_parrots.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Sample Image for Classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare Vision Transformer Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models = {\n",
    "    \"ViT\": \"google/vit-base-patch16-224\",\n",
    "    \"DeiT\": \"facebook/deit-base-patch16-224\",\n",
    "    \"Swin\": \"microsoft/swin-tiny-patch4-window7-224\"\n",
    "}\n",
    "\n",
    "# Classify with each model\n",
    "results = {}\n",
    "\n",
    "for name, model_id in models.items():\n",
    "    print(f\"\\nProcessing with {name}...\")\n",
    "    \n",
    "    processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "    model = AutoModelForImageClassification.from_pretrained(model_id)\n",
    "    \n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Get top 3 predictions\n",
    "    logits = outputs.logits\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    top3 = torch.topk(probs, 3)\n",
    "    \n",
    "    results[name] = []\n",
    "    for i in range(3):\n",
    "        idx = top3.indices[0][i].item()\n",
    "        label = model.config.id2label[idx]\n",
    "        score = top3.values[0][i].item()\n",
    "        results[name].append((label, score))\n",
    "        print(f\"  {i+1}. {label}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison chart\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, (model_name, predictions) in enumerate(results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    labels = [pred[0] for pred in predictions]\n",
    "    scores = [pred[1] for pred in predictions]\n",
    "    \n",
    "    ax.barh(labels, scores)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_title(f'{model_name} Predictions')\n",
    "    \n",
    "    for i, score in enumerate(scores):\n",
    "        ax.text(score + 0.01, i, f'{score:.1%}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Patch-Based Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how images are divided into patches\n",
    "import numpy as np\n",
    "\n",
    "# Create a grid overlay\n",
    "img_array = np.array(image)\n",
    "h, w = img_array.shape[:2]\n",
    "patch_size = 16  # ViT uses 16x16 patches\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Original image\n",
    "ax1.imshow(image)\n",
    "ax1.set_title(\"Original Image\")\n",
    "ax1.axis('off')\n",
    "\n",
    "# Image with patch grid\n",
    "ax2.imshow(image)\n",
    "ax2.set_title(f\"Image Divided into {patch_size}x{patch_size} Patches\")\n",
    "\n",
    "# Draw grid\n",
    "for i in range(0, h, patch_size):\n",
    "    ax2.axhline(y=i, color='red', linewidth=0.5, alpha=0.5)\n",
    "for i in range(0, w, patch_size):\n",
    "    ax2.axvline(x=i, color='red', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "ax2.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total patches: {(h // patch_size) * (w // patch_size)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
