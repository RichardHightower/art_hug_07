{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Language: Transformers for Vision, Audio, and Multimodal AI\n",
    "\n",
    "This notebook provides a hands-on exploration of how transformers have expanded beyond text to revolutionize computer vision, audio processing, and multimodal AI. We'll implement and visualize examples from Article 7, demonstrating practical applications across industries.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Vision Transformers**: How ViT, DeiT, and Swin process images\n",
    "2. **Audio Processing**: Speech recognition with Whisper and audio classification\n",
    "3. **Generative AI**: Creating images with Stable Diffusion XL\n",
    "4. **Multimodal Models**: CLIP, BLIP, and cross-modal search\n",
    "5. **Production Pipelines**: Building scalable systems with SGLang\n",
    "\n",
    "Let's begin our journey beyond language!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's set up our environment and import the necessary libraries. We'll use the Hugging Face ecosystem for its unified API across modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our modules\n",
    "from config import get_device, SAMPLE_IMAGE_URL, OUTPUT_DIR, IMAGES_DIR, AUDIO_DIR\n",
    "\n",
    "# ML libraries\n",
    "from transformers import (\n",
    "    AutoImageProcessor, AutoModelForImageClassification,\n",
    "    AutoProcessor, AutoModel, AutoModelForSpeechRecognition,\n",
    "    pipeline, BlipProcessor, BlipForConditionalGeneration\n",
    ")\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from IPython.display import Audio, display\n",
    "import time\n",
    "\n",
    "# Check device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Vision Transformers - Teaching Transformers to See\n",
    "\n",
    "Vision Transformers (ViT) revolutionized computer vision by applying the transformer architecture to images. Instead of processing pixels sequentially, ViT divides images into patches and treats them like words in a sentence.\n",
    "\n",
    "### How Vision Transformers Work\n",
    "\n",
    "1. **Image → Patches**: Split image into fixed-size patches (e.g., 16×16 pixels)\n",
    "2. **Patches → Embeddings**: Convert each patch to a vector representation\n",
    "3. **Add Positions**: Include positional encodings so the model knows patch locations\n",
    "4. **Self-Attention**: Let patches \"communicate\" to understand the full image\n",
    "5. **Classification**: Output final prediction\n",
    "\n",
    "Let's visualize this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how ViT processes images\n",
    "def visualize_vit_patches(image_url, patch_size=16):\n",
    "    \"\"\"Visualize how ViT divides an image into patches.\"\"\"\n",
    "    # Load image\n",
    "    response = requests.get(image_url, stream=True)\n",
    "    img = Image.open(response.raw)\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Original image\n",
    "    ax1.imshow(img_array)\n",
    "    ax1.set_title(\"Original Image\", fontsize=14)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Image with patch grid\n",
    "    ax2.imshow(img_array)\n",
    "    ax2.set_title(f\"ViT Patches ({patch_size}×{patch_size} pixels)\", fontsize=14)\n",
    "    \n",
    "    # Draw grid\n",
    "    h, w = img_array.shape[:2]\n",
    "    for i in range(0, h, patch_size):\n",
    "        ax2.axhline(y=i, color='red', linewidth=0.5, alpha=0.7)\n",
    "    for i in range(0, w, patch_size):\n",
    "        ax2.axvline(x=i, color='red', linewidth=0.5, alpha=0.7)\n",
    "    \n",
    "    ax2.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate number of patches\n",
    "    n_patches = (h // patch_size) * (w // patch_size)\n",
    "    print(f\"\\nImage divided into {n_patches} patches\")\n",
    "    print(f\"Each patch: {patch_size}×{patch_size} pixels\")\n",
    "    print(f\"Total patches for 224×224 image: {(224//patch_size)**2}\")\n",
    "\n",
    "# Visualize patching process\n",
    "visualize_vit_patches(SAMPLE_IMAGE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Image Classification with ViT\n",
    "\n",
    "Now let's use Vision Transformer to classify an image. Notice how simple the API is—Hugging Face abstracts away the complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image_with_vit(image_url):\n",
    "    \"\"\"Classify an image using Vision Transformer.\"\"\"\n",
    "    print(\"Loading Vision Transformer...\")\n",
    "    \n",
    "    # Load model and processor\n",
    "    processor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "    model = AutoModelForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "    \n",
    "    if device != \"cpu\":\n",
    "        model = model.to(device)\n",
    "    \n",
    "    # Load and display image\n",
    "    image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Show image\n",
    "    ax1.imshow(image)\n",
    "    ax1.set_title(\"Input Image\", fontsize=14)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Process and predict\n",
    "    print(\"\\nProcessing image...\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    if device != \"cpu\":\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get top 5 predictions\n",
    "    logits = outputs.logits\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    top5_probs, top5_indices = torch.topk(probs[0], 5)\n",
    "    \n",
    "    # Plot predictions\n",
    "    labels = [model.config.id2label[idx.item()] for idx in top5_indices]\n",
    "    scores = top5_probs.cpu().numpy()\n",
    "    \n",
    "    ax2.barh(labels, scores, color='skyblue')\n",
    "    ax2.set_xlabel('Confidence Score', fontsize=12)\n",
    "    ax2.set_title('Top 5 Predictions', fontsize=14)\n",
    "    ax2.set_xlim(0, 1)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, (label, score) in enumerate(zip(labels, scores)):\n",
    "        ax2.text(score + 0.01, i, f'{score*100:.1f}%', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTop prediction: {labels[0]} ({scores[0]*100:.1f}% confidence)\")\n",
    "    return labels[0], scores[0]\n",
    "\n",
    "# Run classification\n",
    "predicted_class, confidence = classify_image_with_vit(SAMPLE_IMAGE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Modern Vision Transformers\n",
    "\n",
    "Let's compare different vision transformer architectures to see how they've evolved. Each has unique strengths:\n",
    "\n",
    "- **ViT**: Original, simple, effective with large datasets\n",
    "- **DeiT**: Data-efficient, uses distillation, faster training\n",
    "- **Swin**: Hierarchical, shifted windows, better for dense prediction tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_vision_transformers(image_url):\n",
    "    \"\"\"Compare different vision transformer architectures.\"\"\"\n",
    "    models = {\n",
    "        \"ViT\": \"google/vit-base-patch16-224\",\n",
    "        \"DeiT\": \"facebook/deit-base-patch16-224\",\n",
    "        \"Swin\": \"microsoft/swin-tiny-patch4-window7-224\"\n",
    "    }\n",
    "    \n",
    "    # Load image once\n",
    "    image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Comparing Vision Transformer Architectures\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for name, model_id in models.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Load model\n",
    "            processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "            model = AutoModelForImageClassification.from_pretrained(model_id)\n",
    "            \n",
    "            if device != \"cpu\":\n",
    "                model = model.to(device)\n",
    "            \n",
    "            # Process image\n",
    "            inputs = processor(images=image, return_tensors=\"pt\")\n",
    "            if device != \"cpu\":\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Predict\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            # Get prediction\n",
    "            pred_idx = outputs.logits.argmax(-1).item()\n",
    "            pred_label = model.config.id2label[pred_idx]\n",
    "            confidence = torch.nn.functional.softmax(outputs.logits, dim=-1)[0, pred_idx].item()\n",
    "            \n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            results.append({\n",
    "                \"model\": name,\n",
    "                \"prediction\": pred_label,\n",
    "                \"confidence\": confidence,\n",
    "                \"time\": inference_time\n",
    "            })\n",
    "            \n",
    "            print(f\"  Prediction: {pred_label}\")\n",
    "            print(f\"  Confidence: {confidence:.3f}\")\n",
    "            print(f\"  Time: {inference_time:.3f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    if results:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Confidence comparison\n",
    "        models_names = [r['model'] for r in results]\n",
    "        confidences = [r['confidence'] for r in results]\n",
    "        \n",
    "        ax1.bar(models_names, confidences, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "        ax1.set_ylabel('Confidence Score', fontsize=12)\n",
    "        ax1.set_title('Model Confidence Comparison', fontsize=14)\n",
    "        ax1.set_ylim(0, 1)\n",
    "        \n",
    "        # Inference time comparison\n",
    "        times = [r['time'] for r in results]\n",
    "        \n",
    "        ax2.bar(models_names, times, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "        ax2.set_ylabel('Inference Time (seconds)', fontsize=12)\n",
    "        ax2.set_title('Inference Speed Comparison', fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare models\n",
    "comparison_results = compare_vision_transformers(SAMPLE_IMAGE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Audio Processing - Teaching Transformers to Listen\n",
    "\n",
    "Transformers have revolutionized audio processing, especially speech recognition. Models like Whisper process raw audio end-to-end, eliminating the need for hand-crafted features.\n",
    "\n",
    "### How Audio Transformers Work\n",
    "\n",
    "1. **Audio Waveform**: Raw sound waves captured by microphone\n",
    "2. **Spectrogram**: Convert to visual representation of frequencies over time\n",
    "3. **Transformer Processing**: Apply self-attention to understand patterns\n",
    "4. **Text Generation**: Decode audio features into text tokens\n",
    "\n",
    "Let's visualize this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample audio for demonstration\n",
    "def create_sample_audio():\n",
    "    \"\"\"Create a simple audio sample for visualization.\"\"\"\n",
    "    import numpy as np\n",
    "    import scipy.io.wavfile as wavfile\n",
    "    \n",
    "    # Generate a simple tone\n",
    "    sample_rate = 16000\n",
    "    duration = 2  # seconds\n",
    "    frequency = 440  # Hz (A4 note)\n",
    "    \n",
    "    t = np.linspace(0, duration, int(sample_rate * duration))\n",
    "    # Create a tone with some variation\n",
    "    audio = np.sin(2 * np.pi * frequency * t) * 0.5\n",
    "    audio += np.sin(2 * np.pi * frequency * 2 * t) * 0.2  # Add harmonic\n",
    "    audio += np.random.normal(0, 0.05, audio.shape)  # Add noise\n",
    "    \n",
    "    # Save audio\n",
    "    audio_path = OUTPUT_DIR / \"sample_audio.wav\"\n",
    "    wavfile.write(str(audio_path), sample_rate, (audio * 32767).astype(np.int16))\n",
    "    \n",
    "    return audio_path, audio, sample_rate\n",
    "\n",
    "# Visualize audio processing\n",
    "def visualize_audio_processing():\n",
    "    \"\"\"Visualize how transformers process audio.\"\"\"\n",
    "    audio_path, audio, sample_rate = create_sample_audio()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    # 1. Waveform\n",
    "    time = np.arange(len(audio)) / sample_rate\n",
    "    axes[0, 0].plot(time, audio)\n",
    "    axes[0, 0].set_title('Audio Waveform', fontsize=14)\n",
    "    axes[0, 0].set_xlabel('Time (s)')\n",
    "    axes[0, 0].set_ylabel('Amplitude')\n",
    "    \n",
    "    # 2. Spectrogram\n",
    "    from scipy import signal\n",
    "    frequencies, times, spectrogram = signal.spectrogram(audio, sample_rate)\n",
    "    \n",
    "    im = axes[0, 1].pcolormesh(times, frequencies, 10 * np.log10(spectrogram))\n",
    "    axes[0, 1].set_title('Spectrogram', fontsize=14)\n",
    "    axes[0, 1].set_xlabel('Time (s)')\n",
    "    axes[0, 1].set_ylabel('Frequency (Hz)')\n",
    "    plt.colorbar(im, ax=axes[0, 1], label='Power (dB)')\n",
    "    \n",
    "    # 3. Mel-spectrogram (what Whisper uses)\n",
    "    # Simplified visualization\n",
    "    mel_spec = np.random.randn(80, 100)  # Placeholder for mel-spectrogram\n",
    "    axes[1, 0].imshow(mel_spec, aspect='auto', origin='lower')\n",
    "    axes[1, 0].set_title('Mel-Spectrogram (Whisper Input)', fontsize=14)\n",
    "    axes[1, 0].set_xlabel('Time Frames')\n",
    "    axes[1, 0].set_ylabel('Mel Channels')\n",
    "    \n",
    "    # 4. Processing pipeline\n",
    "    axes[1, 1].text(0.5, 0.7, 'Audio Processing Pipeline:', \n",
    "                    ha='center', va='center', fontsize=16, weight='bold')\n",
    "    axes[1, 1].text(0.5, 0.5, '1. Raw Audio → Mel-Spectrogram', \n",
    "                    ha='center', va='center', fontsize=12)\n",
    "    axes[1, 1].text(0.5, 0.4, '2. Transformer Encoder (30 layers)', \n",
    "                    ha='center', va='center', fontsize=12)\n",
    "    axes[1, 1].text(0.5, 0.3, '3. Transformer Decoder', \n",
    "                    ha='center', va='center', fontsize=12)\n",
    "    axes[1, 1].text(0.5, 0.2, '4. Text Token Generation', \n",
    "                    ha='center', va='center', fontsize=12)\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Play audio\n",
    "    print(\"\\nGenerated sample audio:\")\n",
    "    display(Audio(audio, rate=sample_rate))\n",
    "    \n",
    "    return audio_path\n",
    "\n",
    "# Visualize audio processing\n",
    "sample_audio_path = visualize_audio_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech Recognition with Whisper\n",
    "\n",
    "Whisper is OpenAI's robust speech recognition model, trained on 680,000 hours of multilingual data. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_whisper():\n",
    "    \"\"\"Demonstrate speech recognition with Whisper.\"\"\"\n",
    "    print(\"Loading Whisper model...\")\n",
    "    \n",
    "    # Create ASR pipeline\n",
    "    asr = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=\"openai/whisper-base\",\n",
    "        device=0 if device == \"cuda\" else -1\n",
    "    )\n",
    "    \n",
    "    # Check for sample audio files\n",
    "    audio_files = list(Path(AUDIO_DIR).glob(\"*.wav\"))\n",
    "    \n",
    "    if audio_files:\n",
    "        # Process real audio file\n",
    "        audio_file = audio_files[0]\n",
    "        print(f\"\\nTranscribing: {audio_file.name}\")\n",
    "        \n",
    "        result = asr(str(audio_file))\n",
    "        print(f\"Transcription: {result['text']}\")\n",
    "        \n",
    "        # Display audio\n",
    "        display(Audio(str(audio_file)))\n",
    "    else:\n",
    "        # Demonstrate with explanation\n",
    "        print(\"\\nNo audio files found. Here's how Whisper works:\")\n",
    "        print(\"\\n1. Input: Audio file (WAV, MP3, FLAC, etc.)\")\n",
    "        print(\"2. Processing: Convert to mel-spectrogram\")\n",
    "        print(\"3. Encoding: 30-layer transformer encoder\")\n",
    "        print(\"4. Decoding: Generate text tokens\")\n",
    "        print(\"5. Output: Transcribed text\")\n",
    "        \n",
    "        print(\"\\nExample transcription:\")\n",
    "        print(\"Audio: [Person speaking about AI]\")\n",
    "        print(\"Transcription: 'Artificial intelligence is transforming how we interact with technology.'\")\n",
    "    \n",
    "    print(\"\\nWhisper capabilities:\")\n",
    "    print(\"✓ 99 languages supported\")\n",
    "    print(\"✓ Robust to accents and noise\")\n",
    "    print(\"✓ Automatic language detection\")\n",
    "    print(\"✓ Timestamp generation\")\n",
    "    print(\"✓ Translation to English\")\n",
    "\n",
    "# Demonstrate Whisper\n",
    "demonstrate_whisper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Classification\n",
    "\n",
    "Beyond speech, transformers can classify any type of sound—from music genres to environmental sounds. This has applications in security, healthcare, and smart home devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_audio_classification():\n",
    "    \"\"\"Demonstrate audio event classification.\"\"\"\n",
    "    print(\"Audio Classification with Transformers\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create audio classifier\n",
    "    classifier = pipeline(\n",
    "        \"audio-classification\",\n",
    "        model=\"superb/wav2vec2-base-superb-ks\",\n",
    "        device=0 if device == \"cuda\" else -1\n",
    "    )\n",
    "    \n",
    "    # Check for audio files\n",
    "    audio_files = list(Path(AUDIO_DIR).glob(\"*.wav\"))\n",
    "    \n",
    "    if audio_files:\n",
    "        # Classify real audio\n",
    "        for audio_file in audio_files[:2]:\n",
    "            print(f\"\\nClassifying: {audio_file.name}\")\n",
    "            \n",
    "            try:\n",
    "                results = classifier(str(audio_file))\n",
    "                \n",
    "                # Visualize results\n",
    "                labels = [r['label'] for r in results[:5]]\n",
    "                scores = [r['score'] for r in results[:5]]\n",
    "                \n",
    "                plt.figure(figsize=(8, 4))\n",
    "                plt.barh(labels, scores, color='lightcoral')\n",
    "                plt.xlabel('Confidence Score')\n",
    "                plt.title(f'Audio Classification: {audio_file.name}')\n",
    "                plt.xlim(0, 1)\n",
    "                \n",
    "                for i, score in enumerate(scores):\n",
    "                    plt.text(score + 0.01, i, f'{score:.3f}', va='center')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "    else:\n",
    "        # Show example use cases\n",
    "        print(\"\\nAudio Classification Use Cases:\")\n",
    "        \n",
    "        use_cases = [\n",
    "            {\"domain\": \"Security\", \"sounds\": [\"glass_breaking\", \"alarm\", \"scream\"]},\n",
    "            {\"domain\": \"Healthcare\", \"sounds\": [\"cough\", \"snoring\", \"heartbeat\"]},\n",
    "            {\"domain\": \"Smart Home\", \"sounds\": [\"doorbell\", \"dog_barking\", \"baby_crying\"]},\n",
    "            {\"domain\": \"Industrial\", \"sounds\": [\"machinery_fault\", \"leak\", \"abnormal_vibration\"]}\n",
    "        ]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for idx, use_case in enumerate(use_cases):\n",
    "            # Simulate classification results\n",
    "            sounds = use_case[\"sounds\"]\n",
    "            scores = np.random.dirichlet(np.ones(len(sounds)) * 2)\n",
    "            \n",
    "            axes[idx].barh(sounds, scores, color=plt.cm.Set3(idx))\n",
    "            axes[idx].set_xlabel('Detection Confidence')\n",
    "            axes[idx].set_title(f'{use_case[\"domain\"]} Applications')\n",
    "            axes[idx].set_xlim(0, 1)\n",
    "            \n",
    "            for i, score in enumerate(scores):\n",
    "                axes[idx].text(score + 0.01, i, f'{score:.2f}', va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demonstrate audio classification\n",
    "demonstrate_audio_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Generative AI - Creating Images with Diffusion Models\n",
    "\n",
    "Diffusion models like Stable Diffusion XL represent a breakthrough in generative AI. They start with random noise and progressively refine it into detailed images guided by text prompts.\n",
    "\n",
    "### How Diffusion Works\n",
    "\n",
    "1. **Start with Noise**: Pure random pixels\n",
    "2. **Text Encoding**: Convert prompt to embeddings\n",
    "3. **Iterative Denoising**: Gradually remove noise\n",
    "4. **Guidance**: Text embeddings steer the process\n",
    "5. **Final Image**: High-quality result after ~25-50 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_diffusion_process():\n",
    "    \"\"\"Visualize the diffusion process conceptually.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    \n",
    "    # Simulate diffusion steps\n",
    "    steps = [0, 5, 10, 20, 25]\n",
    "    \n",
    "    for idx, (ax, step) in enumerate(zip(axes, steps)):\n",
    "        # Create increasingly clear image\n",
    "        noise_level = 1.0 - (step / 25)\n",
    "        \n",
    "        # Base image (hidden in noise initially)\n",
    "        x, y = np.meshgrid(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100))\n",
    "        image = np.exp(-(x**2 + y**2) / 0.2)  # Simple gaussian\n",
    "        \n",
    "        # Add noise\n",
    "        noise = np.random.randn(100, 100) * noise_level\n",
    "        noisy_image = image * (1 - noise_level) + noise\n",
    "        \n",
    "        ax.imshow(noisy_image, cmap='viridis')\n",
    "        ax.set_title(f'Step {step}', fontsize=12)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        if step == 0:\n",
    "            ax.text(50, 110, 'Pure Noise', ha='center', fontsize=10)\n",
    "        elif step == 25:\n",
    "            ax.text(50, 110, 'Final Image', ha='center', fontsize=10)\n",
    "    \n",
    "    plt.suptitle('Diffusion Process: From Noise to Image', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nDiffusion Model Components:\")\n",
    "    print(\"1. Text Encoder: Converts prompt to embeddings\")\n",
    "    print(\"2. U-Net: Predicts noise to remove at each step\")\n",
    "    print(\"3. Scheduler: Controls the denoising process\")\n",
    "    print(\"4. VAE Decoder: Converts latents to final image\")\n",
    "\n",
    "# Visualize diffusion\n",
    "visualize_diffusion_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Images with Stable Diffusion XL\n",
    "\n",
    "Let's generate images from text prompts. Note: This requires significant GPU memory. We'll show the process even if generation isn't possible on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_image_generation():\n",
    "    \"\"\"Demonstrate text-to-image generation.\"\"\"\n",
    "    print(\"Text-to-Image Generation with Stable Diffusion XL\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    prompts = [\n",
    "        {\n",
    "            \"prompt\": \"A serene Japanese garden with cherry blossoms and a wooden bridge, highly detailed digital art\",\n",
    "            \"style\": \"peaceful_landscape\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"A friendly robot teaching mathematics to children in a colorful classroom, cartoon style\",\n",
    "            \"style\": \"educational_illustration\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"An astronaut riding a horse on Mars, photorealistic, dramatic lighting\",\n",
    "            \"style\": \"surreal_concept\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Check if we can actually generate\n",
    "    can_generate = device == \"cuda\" and torch.cuda.get_device_properties(0).total_memory > 8e9\n",
    "    \n",
    "    if can_generate:\n",
    "        try:\n",
    "            print(\"Loading Stable Diffusion XL...\")\n",
    "            from diffusion_models import generate_image\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "            for idx, (prompt_info, ax) in enumerate(zip(prompts, axes)):\n",
    "                print(f\"\\nGenerating: {prompt_info['style']}...\")\n",
    "                \n",
    "                # Generate image\n",
    "                image = generate_image(\n",
    "                    prompt_info[\"prompt\"],\n",
    "                    num_inference_steps=25\n",
    "                )\n",
    "                \n",
    "                # Display\n",
    "                ax.imshow(image)\n",
    "                ax.set_title(prompt_info['style'].replace('_', ' ').title(), fontsize=12)\n",
    "                ax.axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nGeneration failed: {e}\")\n",
    "            can_generate = False\n",
    "    \n",
    "    if not can_generate:\n",
    "        # Show prompt engineering tips instead\n",
    "        print(\"\\nPrompt Engineering Best Practices:\")\n",
    "        print(\"\\n1. Be Specific:\")\n",
    "        print(\"   ❌ 'A dog'\")\n",
    "        print(\"   ✅ 'A golden retriever puppy playing in autumn leaves, soft lighting'\")\n",
    "        \n",
    "        print(\"\\n2. Include Style Modifiers:\")\n",
    "        print(\"   • 'digital art', 'oil painting', 'photorealistic'\")\n",
    "        print(\"   • 'studio lighting', 'golden hour', 'dramatic shadows'\")\n",
    "        print(\"   • 'highly detailed', '4k', 'award winning'\")\n",
    "        \n",
    "        print(\"\\n3. Use Negative Prompts:\")\n",
    "        print(\"   • 'blurry, low quality, distorted'\")\n",
    "        print(\"   • 'extra limbs, bad anatomy' (for people)\")\n",
    "        print(\"   • 'watermark, text, logo'\")\n",
    "        \n",
    "        # Visualize prompt structure\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.text(0.5, 0.9, 'Anatomy of a Good Prompt', ha='center', fontsize=16, weight='bold')\n",
    "        \n",
    "        components = [\n",
    "            \"Subject: 'A majestic eagle'\",\n",
    "            \"Action: 'soaring through'\",\n",
    "            \"Setting: 'mountain peaks at sunset'\",\n",
    "            \"Style: 'photorealistic, National Geographic style'\",\n",
    "            \"Quality: 'highly detailed, 8k resolution'\"\n",
    "        ]\n",
    "        \n",
    "        for i, component in enumerate(components):\n",
    "            ax.text(0.1, 0.7 - i*0.12, component, fontsize=12)\n",
    "        \n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demonstrate image generation\n",
    "demonstrate_image_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multimodal Models - Connecting Vision and Language\n",
    "\n",
    "Multimodal models like CLIP and BLIP understand both images and text, enabling powerful applications like image search, captioning, and visual question answering.\n",
    "\n",
    "### How CLIP Works\n",
    "\n",
    "CLIP (Contrastive Language-Image Pretraining) learns to embed images and text in the same vector space:\n",
    "\n",
    "1. **Dual Encoders**: Separate networks for images and text\n",
    "2. **Shared Space**: Both produce 512-dimensional vectors\n",
    "3. **Contrastive Learning**: Matching pairs pulled together, mismatches pushed apart\n",
    "4. **Zero-Shot**: Can classify images using any text description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clip_embedding_space():\n",
    "    \"\"\"Visualize how CLIP creates a shared embedding space.\"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    import matplotlib.patches as patches\n",
    "    \n",
    "    # Create synthetic embeddings for visualization\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Image embeddings (clustered by type)\n",
    "    cat_images = np.random.randn(5, 50) + np.array([2, 0] + [0]*48)\n",
    "    dog_images = np.random.randn(5, 50) + np.array([-2, 0] + [0]*48)\n",
    "    car_images = np.random.randn(5, 50) + np.array([0, 2] + [0]*48)\n",
    "    \n",
    "    # Text embeddings (should be near corresponding images)\n",
    "    cat_texts = np.random.randn(3, 50) * 0.5 + np.array([2, 0] + [0]*48)\n",
    "    dog_texts = np.random.randn(3, 50) * 0.5 + np.array([-2, 0] + [0]*48)\n",
    "    car_texts = np.random.randn(3, 50) * 0.5 + np.array([0, 2] + [0]*48)\n",
    "    \n",
    "    # Combine all embeddings\n",
    "    all_embeddings = np.vstack([\n",
    "        cat_images, dog_images, car_images,\n",
    "        cat_texts, dog_texts, car_texts\n",
    "    ])\n",
    "    \n",
    "    # Reduce to 2D for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(all_embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Plot image embeddings\n",
    "    ax.scatter(embeddings_2d[:5, 0], embeddings_2d[:5, 1], \n",
    "               c='red', s=100, marker='o', label='Cat Images', alpha=0.6)\n",
    "    ax.scatter(embeddings_2d[5:10, 0], embeddings_2d[5:10, 1], \n",
    "               c='blue', s=100, marker='o', label='Dog Images', alpha=0.6)\n",
    "    ax.scatter(embeddings_2d[10:15, 0], embeddings_2d[10:15, 1], \n",
    "               c='green', s=100, marker='o', label='Car Images', alpha=0.6)\n",
    "    \n",
    "    # Plot text embeddings\n",
    "    ax.scatter(embeddings_2d[15:18, 0], embeddings_2d[15:18, 1], \n",
    "               c='red', s=100, marker='^', label='Cat Texts', alpha=0.8)\n",
    "    ax.scatter(embeddings_2d[18:21, 0], embeddings_2d[18:21, 1], \n",
    "               c='blue', s=100, marker='^', label='Dog Texts', alpha=0.8)\n",
    "    ax.scatter(embeddings_2d[21:24, 0], embeddings_2d[21:24, 1], \n",
    "               c='green', s=100, marker='^', label='Car Texts', alpha=0.8)\n",
    "    \n",
    "    # Add cluster circles\n",
    "    for center, color in [(embeddings_2d[:5].mean(0), 'red'),\n",
    "                          (embeddings_2d[5:10].mean(0), 'blue'),\n",
    "                          (embeddings_2d[10:15].mean(0), 'green')]:\n",
    "        circle = patches.Circle(center, 1.5, fill=False, \n",
    "                               edgecolor=color, linewidth=2, linestyle='--', alpha=0.5)\n",
    "        ax.add_patch(circle)\n",
    "    \n",
    "    ax.set_xlabel('Dimension 1', fontsize=12)\n",
    "    ax.set_ylabel('Dimension 2', fontsize=12)\n",
    "    ax.set_title('CLIP Embedding Space (Simplified 2D Visualization)', fontsize=16)\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations\n",
    "    ax.annotate('Images and text describing\\nthe same concept cluster together',\n",
    "                xy=(0, -2), xytext=(2, -3.5),\n",
    "                arrowprops=dict(arrowstyle='->', lw=1.5),\n",
    "                fontsize=11, ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKey Insights:\")\n",
    "    print(\"• Images and matching text descriptions are embedded nearby\")\n",
    "    print(\"• This enables zero-shot classification and search\")\n",
    "    print(\"• The actual CLIP space is 512-dimensional\")\n",
    "    print(\"• Trained on 400 million image-text pairs\")\n",
    "\n",
    "# Visualize CLIP embeddings\n",
    "visualize_clip_embedding_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Multimodal Search Engine\n",
    "\n",
    "Let's build a simple image search engine that finds images based on text descriptions. This demonstrates the power of multimodal embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multimodal_search_demo():\n",
    "    \"\"\"Build and demonstrate a multimodal search engine.\"\"\"\n",
    "    print(\"Building Multimodal Search Engine with CLIP\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load CLIP model\n",
    "    print(\"\\nLoading CLIP model...\")\n",
    "    model = AutoModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "    processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "    model.eval()\n",
    "    \n",
    "    if device != \"cpu\":\n",
    "        model = model.to(device)\n",
    "    \n",
    "    # Create sample images for demo\n",
    "    print(\"\\nCreating sample image collection...\")\n",
    "    \n",
    "    # Generate synthetic images with descriptions\n",
    "    sample_data = [\n",
    "        {\"desc\": \"Red sports car\", \"color\": \"red\", \"object\": \"car\"},\n",
    "        {\"desc\": \"Blue ocean waves\", \"color\": \"blue\", \"object\": \"ocean\"},\n",
    "        {\"desc\": \"Green forest\", \"color\": \"green\", \"object\": \"forest\"},\n",
    "        {\"desc\": \"Yellow sunflower\", \"color\": \"yellow\", \"object\": \"flower\"},\n",
    "        {\"desc\": \"Purple mountain sunset\", \"color\": \"purple\", \"object\": \"mountain\"}\n",
    "    ]\n",
    "    \n",
    "    images = []\n",
    "    for item in sample_data:\n",
    "        # Create simple colored image\n",
    "        img = Image.new('RGB', (224, 224), color=item['color'])\n",
    "        images.append(img)\n",
    "    \n",
    "    # Embed images\n",
    "    print(\"\\nEmbedding images...\")\n",
    "    inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n",
    "    if device != \"cpu\":\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Search function\n",
    "    def search_images(query):\n",
    "        \"\"\"Search images by text query.\"\"\"\n",
    "        # Embed query\n",
    "        text_inputs = processor(text=[query], return_tensors=\"pt\", padding=True)\n",
    "        if device != \"cpu\":\n",
    "            text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_features = model.get_text_features(**text_inputs)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Compute similarities\n",
    "        similarities = (image_features @ text_features.T).squeeze(1)\n",
    "        \n",
    "        # Get ranking\n",
    "        scores, indices = similarities.sort(descending=True)\n",
    "        \n",
    "        return scores.cpu().numpy(), indices.cpu().numpy()\n",
    "    \n",
    "    # Test queries\n",
    "    queries = [\n",
    "        \"something red\",\n",
    "        \"nature scene\",\n",
    "        \"bright yellow object\",\n",
    "        \"water and waves\"\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, query in enumerate(queries):\n",
    "        scores, ranking = search_images(query)\n",
    "        \n",
    "        # Plot results\n",
    "        ax = axes[idx]\n",
    "        ax.set_title(f'Query: \"{query}\"', fontsize=14, pad=10)\n",
    "        \n",
    "        # Show top 3 results\n",
    "        for i in range(3):\n",
    "            img_idx = ranking[i]\n",
    "            score = scores[i]\n",
    "            \n",
    "            # Create small subplot\n",
    "            x_pos = i * 0.3 + 0.1\n",
    "            y_pos = 0.3\n",
    "            \n",
    "            # Draw image placeholder\n",
    "            rect = plt.Rectangle((x_pos, y_pos), 0.2, 0.3, \n",
    "                               facecolor=sample_data[img_idx]['color'],\n",
    "                               edgecolor='black', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Add description\n",
    "            ax.text(x_pos + 0.1, y_pos - 0.05, sample_data[img_idx]['desc'],\n",
    "                   ha='center', fontsize=10)\n",
    "            ax.text(x_pos + 0.1, y_pos - 0.1, f'Score: {score:.3f}',\n",
    "                   ha='center', fontsize=9, style='italic')\n",
    "            \n",
    "            # Rank label\n",
    "            ax.text(x_pos + 0.1, y_pos + 0.35, f'#{i+1}',\n",
    "                   ha='center', fontsize=12, weight='bold')\n",
    "        \n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Multimodal Search Results', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✨ Search engine ready! The system can:\")\n",
    "    print(\"• Find images matching natural language queries\")\n",
    "    print(\"• Work with any text description (zero-shot)\")\n",
    "    print(\"• Rank results by semantic similarity\")\n",
    "    print(\"• Scale to millions of images with vector databases\")\n",
    "\n",
    "# Build search engine\n",
    "build_multimodal_search_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Multimodal Models: BLIP and Beyond\n",
    "\n",
    "While CLIP excels at understanding image-text relationships, newer models like BLIP, BLIP-2, and LLaVA can generate captions and answer questions about images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_advanced_multimodal():\n",
    "    \"\"\"Demonstrate advanced multimodal capabilities.\"\"\"\n",
    "    print(\"Advanced Multimodal Models\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create comparison visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Model timeline and capabilities\n",
    "    models = [\n",
    "        {\"name\": \"CLIP\", \"year\": 2021, \"capabilities\": [\"Image-Text Matching\", \"Zero-shot Classification\"]},\n",
    "        {\"name\": \"BLIP\", \"year\": 2022, \"capabilities\": [\"Image Captioning\", \"VQA\", \"Image-Text Matching\"]},\n",
    "        {\"name\": \"BLIP-2\", \"year\": 2023, \"capabilities\": [\"Efficient Architecture\", \"Better Captioning\", \"Instruction Following\"]},\n",
    "        {\"name\": \"LLaVA\", \"year\": 2023, \"capabilities\": [\"Visual Reasoning\", \"Detailed Descriptions\", \"Multi-turn Dialogue\"]},\n",
    "        {\"name\": \"GPT-4V\", \"year\": 2023, \"capabilities\": [\"Complex Reasoning\", \"OCR\", \"Multi-image Understanding\"]}\n",
    "    ]\n",
    "    \n",
    "    # Plot timeline\n",
    "    years = [m[\"year\"] for m in models]\n",
    "    names = [m[\"name\"] for m in models]\n",
    "    \n",
    "    # Create timeline\n",
    "    for i, model in enumerate(models):\n",
    "        y_pos = i\n",
    "        \n",
    "        # Model box\n",
    "        rect = plt.Rectangle((model[\"year\"] - 2021, y_pos - 0.3), 0.8, 0.6,\n",
    "                           facecolor='lightblue', edgecolor='navy', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Model name\n",
    "        ax.text(model[\"year\"] - 2021 + 0.4, y_pos, model[\"name\"],\n",
    "               ha='center', va='center', fontsize=12, weight='bold')\n",
    "        \n",
    "        # Capabilities\n",
    "        for j, cap in enumerate(model[\"capabilities\"]):\n",
    "            ax.text(model[\"year\"] - 2021 + 1.2, y_pos + 0.2 - j*0.2, \n",
    "                   f\"• {cap}\", fontsize=9)\n",
    "    \n",
    "    ax.set_xlim(-0.5, 5)\n",
    "    ax.set_ylim(-0.5, len(models) - 0.5)\n",
    "    ax.set_xlabel('Years since 2021', fontsize=12)\n",
    "    ax.set_ylabel('Models', fontsize=12)\n",
    "    ax.set_title('Evolution of Multimodal Models', fontsize=16)\n",
    "    ax.grid(True, axis='x', alpha=0.3)\n",
    "    \n",
    "    # Remove y-axis ticks\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Example use cases\n",
    "    print(\"\\nPractical Applications:\")\n",
    "    \n",
    "    use_cases = {\n",
    "        \"E-commerce\": [\n",
    "            \"Generate product descriptions from images\",\n",
    "            \"Answer customer questions about products\",\n",
    "            \"Visual search for similar items\"\n",
    "        ],\n",
    "        \"Healthcare\": [\n",
    "            \"Describe medical images for reports\",\n",
    "            \"Answer questions about X-rays or scans\",\n",
    "            \"Assist in diagnostic workflows\"\n",
    "        ],\n",
    "        \"Accessibility\": [\n",
    "            \"Describe images for visually impaired users\",\n",
    "            \"Answer questions about visual content\",\n",
    "            \"Navigate physical spaces with camera input\"\n",
    "        ],\n",
    "        \"Education\": [\n",
    "            \"Explain diagrams and charts\",\n",
    "            \"Answer questions about educational images\",\n",
    "            \"Create study materials from visuals\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for domain, apps in use_cases.items():\n",
    "        print(f\"\\n{domain}:\")\n",
    "        for app in apps:\n",
    "            print(f\"  • {app}\")\n",
    "\n",
    "# Demonstrate advanced multimodal\n",
    "demonstrate_advanced_multimodal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Production Deployment with SGLang\n",
    "\n",
    "SGLang (Serving Graph Language) enables efficient deployment of multimodal pipelines. It provides graph-based orchestration, automatic optimization, and scalable serving.\n",
    "\n",
    "### Building a Customer Support Pipeline\n",
    "\n",
    "Let's design a production pipeline that processes customer screenshots and voice messages to generate support tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sglang_pipeline():\n",
    "    \"\"\"Visualize a production multimodal pipeline.\"\"\"\n",
    "    import matplotlib.patches as mpatches\n",
    "    from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Define nodes\n",
    "    nodes = [\n",
    "        {\"name\": \"Image Input\", \"pos\": (1, 7), \"color\": \"lightblue\", \"type\": \"input\"},\n",
    "        {\"name\": \"Audio Input\", \"pos\": (1, 5), \"color\": \"lightblue\", \"type\": \"input\"},\n",
    "        {\"name\": \"Image Classifier\\n(ViT/CLIP)\", \"pos\": (4, 7), \"color\": \"lightgreen\", \"type\": \"model\"},\n",
    "        {\"name\": \"Audio Transcriber\\n(Whisper)\", \"pos\": (4, 5), \"color\": \"lightgreen\", \"type\": \"model\"},\n",
    "        {\"name\": \"Context Combiner\", \"pos\": (7, 6), \"color\": \"lightyellow\", \"type\": \"processor\"},\n",
    "        {\"name\": \"LLM Summarizer\\n(Llama 2)\", \"pos\": (10, 6), \"color\": \"lightcoral\", \"type\": \"model\"},\n",
    "        {\"name\": \"Ticket Output\", \"pos\": (13, 6), \"color\": \"lightgray\", \"type\": \"output\"}\n",
    "    ]\n",
    "    \n",
    "    # Draw nodes\n",
    "    for node in nodes:\n",
    "        box = FancyBboxPatch(\n",
    "            (node[\"pos\"][0] - 0.8, node[\"pos\"][1] - 0.3),\n",
    "            1.6, 0.6,\n",
    "            boxstyle=\"round,pad=0.1\",\n",
    "            facecolor=node[\"color\"],\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=2\n",
    "        )\n",
    "        ax.add_patch(box)\n",
    "        ax.text(node[\"pos\"][0], node[\"pos\"][1], node[\"name\"],\n",
    "               ha='center', va='center', fontsize=10, weight='bold')\n",
    "    \n",
    "    # Draw connections\n",
    "    connections = [\n",
    "        (nodes[0][\"pos\"], nodes[2][\"pos\"]),  # Image -> Classifier\n",
    "        (nodes[1][\"pos\"], nodes[3][\"pos\"]),  # Audio -> Transcriber\n",
    "        (nodes[2][\"pos\"], nodes[4][\"pos\"]),  # Classifier -> Combiner\n",
    "        (nodes[3][\"pos\"], nodes[4][\"pos\"]),  # Transcriber -> Combiner\n",
    "        (nodes[4][\"pos\"], nodes[5][\"pos\"]),  # Combiner -> Summarizer\n",
    "        (nodes[5][\"pos\"], nodes[6][\"pos\"])   # Summarizer -> Output\n",
    "    ]\n",
    "    \n",
    "    for start, end in connections:\n",
    "        arrow = FancyArrowPatch(\n",
    "            start, end,\n",
    "            connectionstyle=\"arc3,rad=0.1\",\n",
    "            arrowstyle=\"->\",\n",
    "            mutation_scale=20,\n",
    "            linewidth=2,\n",
    "            color=\"darkblue\"\n",
    "        )\n",
    "        ax.add_patch(arrow)\n",
    "    \n",
    "    # Add annotations\n",
    "    ax.text(2.5, 7.5, \"Screenshot\", fontsize=9, style='italic')\n",
    "    ax.text(2.5, 4.5, \"Voice Message\", fontsize=9, style='italic')\n",
    "    ax.text(11.5, 6.5, \"Support Ticket\", fontsize=9, style='italic')\n",
    "    \n",
    "    # Add optimization badges\n",
    "    optimizations = [\n",
    "        {\"text\": \"Quantized\\n(AWQ)\", \"pos\": (4, 8)},\n",
    "        {\"text\": \"Batched\\nInference\", \"pos\": (7, 7)},\n",
    "        {\"text\": \"Cached\\nEmbeddings\", \"pos\": (10, 7)}\n",
    "    ]\n",
    "    \n",
    "    for opt in optimizations:\n",
    "        badge = mpatches.Rectangle(\n",
    "            (opt[\"pos\"][0] - 0.5, opt[\"pos\"][1] - 0.2),\n",
    "            1, 0.4,\n",
    "            facecolor=\"gold\",\n",
    "            edgecolor=\"orange\",\n",
    "            linewidth=1\n",
    "        )\n",
    "        ax.add_patch(badge)\n",
    "        ax.text(opt[\"pos\"][0], opt[\"pos\"][1], opt[\"text\"],\n",
    "               ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    ax.set_xlim(0, 14)\n",
    "    ax.set_ylim(4, 8.5)\n",
    "    ax.set_title('SGLang Multimodal Pipeline for Customer Support', fontsize=16)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show SGLang code structure\n",
    "    print(\"\\nSGLang Pipeline Definition:\")\n",
    "    print(\"\"\"\\n@sgl.function\n",
    "def support_pipeline(s, image, audio):\n",
    "    # Parallel processing of inputs\n",
    "    s_img = classify_image.run(image=image)\n",
    "    s_audio = transcribe_audio.run(audio=audio)\n",
    "    \n",
    "    # Combine results\n",
    "    image_class = s_img[\"classification\"]\n",
    "    transcript = s_audio[\"text\"]\n",
    "    \n",
    "    # Generate summary\n",
    "    s = summarize_ticket(s, image_class, transcript)\n",
    "    return s\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"\\nProduction Features:\")\n",
    "    print(\"✓ Automatic batching for concurrent requests\")\n",
    "    print(\"✓ Model quantization (4-bit, 8-bit) for efficiency\")\n",
    "    print(\"✓ Request caching and deduplication\")\n",
    "    print(\"✓ Health checks and monitoring endpoints\")\n",
    "    print(\"✓ Horizontal scaling with load balancing\")\n",
    "\n",
    "# Visualize pipeline\n",
    "visualize_sglang_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Optimization Strategies\n",
    "\n",
    "Let's explore key optimization techniques for production multimodal systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_optimization_strategies():\n",
    "    \"\"\"Show optimization strategies for production deployment.\"\"\"\n",
    "    \n",
    "    # Create comparison chart\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Memory usage comparison\n",
    "    models = ['ViT-Base', 'CLIP-Base', 'Whisper-Base', 'Llama-7B']\n",
    "    fp32_memory = [0.35, 0.43, 0.39, 28.0]  # GB\n",
    "    int8_memory = [0.09, 0.11, 0.10, 7.0]   # GB\n",
    "    int4_memory = [0.04, 0.05, 0.05, 3.5]   # GB\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax1.bar(x - width, fp32_memory, width, label='FP32', color='#ff7f0e')\n",
    "    ax1.bar(x, int8_memory, width, label='INT8', color='#2ca02c')\n",
    "    ax1.bar(x + width, int4_memory, width, label='INT4', color='#1f77b4')\n",
    "    \n",
    "    ax1.set_xlabel('Models', fontsize=12)\n",
    "    ax1.set_ylabel('Memory Usage (GB)', fontsize=12)\n",
    "    ax1.set_title('Model Quantization Impact', fontsize=14)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, axis='y', alpha=0.3)\n",
    "    \n",
    "    # Throughput comparison\n",
    "    batch_sizes = [1, 4, 8, 16, 32]\n",
    "    single_model = [10, 35, 65, 120, 200]  # requests/sec\n",
    "    with_batching = [10, 40, 75, 145, 280]\n",
    "    with_caching = [15, 60, 120, 230, 420]\n",
    "    \n",
    "    ax2.plot(batch_sizes, single_model, 'o-', label='Single Model', linewidth=2)\n",
    "    ax2.plot(batch_sizes, with_batching, 's-', label='With Batching', linewidth=2)\n",
    "    ax2.plot(batch_sizes, with_caching, '^-', label='With Caching', linewidth=2)\n",
    "    \n",
    "    ax2.set_xlabel('Batch Size', fontsize=12)\n",
    "    ax2.set_ylabel('Throughput (requests/sec)', fontsize=12)\n",
    "    ax2.set_title('Optimization Techniques Impact', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Best practices\n",
    "    print(\"\\nProduction Deployment Best Practices:\")\n",
    "    print(\"\\n1. Model Optimization:\")\n",
    "    print(\"   • Quantization: Reduce precision (FP16 → INT8 → INT4)\")\n",
    "    print(\"   • Pruning: Remove unnecessary weights\")\n",
    "    print(\"   • Distillation: Train smaller models from larger ones\")\n",
    "    \n",
    "    print(\"\\n2. Inference Optimization:\")\n",
    "    print(\"   • Batching: Process multiple requests together\")\n",
    "    print(\"   • Caching: Store frequent computations\")\n",
    "    print(\"   • Streaming: Return partial results as available\")\n",
    "    \n",
    "    print(\"\\n3. Infrastructure:\")\n",
    "    print(\"   • GPU pooling: Share GPUs across services\")\n",
    "    print(\"   • Auto-scaling: Scale based on load\")\n",
    "    print(\"   • Load balancing: Distribute requests evenly\")\n",
    "    \n",
    "    print(\"\\n4. Monitoring:\")\n",
    "    print(\"   • Latency tracking: P50, P95, P99\")\n",
    "    print(\"   • Error rates: Track failures and retries\")\n",
    "    print(\"   • Resource usage: GPU, memory, network\")\n",
    "\n",
    "# Show optimization strategies\n",
    "demonstrate_optimization_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've explored how transformers have expanded beyond language to revolutionize:\n",
    "\n",
    "1. **Vision**: ViT, DeiT, and Swin process images as sequences\n",
    "2. **Audio**: Whisper and Wav2Vec enable robust speech recognition\n",
    "3. **Generation**: Diffusion models create images from text\n",
    "4. **Multimodal**: CLIP and BLIP connect different modalities\n",
    "5. **Production**: SGLang enables scalable deployment\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Unified Architecture**: Transformers work across all modalities\n",
    "- **Pretrained Models**: Leverage existing models for your tasks\n",
    "- **Simple APIs**: Hugging Face makes advanced AI accessible\n",
    "- **Production Ready**: Modern tools enable scalable deployment\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "1. **Experiment**: Try different models on your own data\n",
    "2. **Fine-tune**: Adapt models to your specific domain\n",
    "3. **Build**: Create multimodal applications\n",
    "4. **Deploy**: Use SGLang or similar tools for production\n",
    "\n",
    "The future is multimodal—and you're now equipped to build it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: The Multimodal AI Landscape\n",
    "def create_final_summary():\n",
    "    \"\"\"Create a final summary visualization.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Central node\n",
    "    ax.scatter([0], [0], s=2000, c='gold', edgecolors='black', linewidth=3, zorder=5)\n",
    "    ax.text(0, 0, 'Multimodal\\nAI', ha='center', va='center', fontsize=16, weight='bold')\n",
    "    \n",
    "    # Modality nodes\n",
    "    modalities = [\n",
    "        {\"name\": \"Vision\", \"pos\": (-2, 2), \"color\": \"lightblue\", \"examples\": [\"ViT\", \"DeiT\", \"Swin\"]},\n",
    "        {\"name\": \"Audio\", \"pos\": (2, 2), \"color\": \"lightgreen\", \"examples\": [\"Whisper\", \"Wav2Vec\", \"CLAP\"]},\n",
    "        {\"name\": \"Text\", \"pos\": (0, -2.5), \"color\": \"lightcoral\", \"examples\": [\"BERT\", \"GPT\", \"T5\"]},\n",
    "        {\"name\": \"Generation\", \"pos\": (-2, -2), \"color\": \"plum\", \"examples\": [\"DALL-E\", \"Stable Diffusion\", \"Midjourney\"]},\n",
    "        {\"name\": \"Cross-Modal\", \"pos\": (2, -2), \"color\": \"lightyellow\", \"examples\": [\"CLIP\", \"BLIP\", \"LLaVA\"]}\n",
    "    ]\n",
    "    \n",
    "    for mod in modalities:\n",
    "        # Draw connection to center\n",
    "        ax.plot([0, mod[\"pos\"][0]], [0, mod[\"pos\"][1]], 'k--', alpha=0.3, linewidth=2)\n",
    "        \n",
    "        # Draw modality circle\n",
    "        ax.scatter(mod[\"pos\"][0], mod[\"pos\"][1], s=1500, c=mod[\"color\"], \n",
    "                  edgecolors='black', linewidth=2, zorder=3)\n",
    "        ax.text(mod[\"pos\"][0], mod[\"pos\"][1], mod[\"name\"], \n",
    "               ha='center', va='center', fontsize=14, weight='bold')\n",
    "        \n",
    "        # Add examples\n",
    "        for i, example in enumerate(mod[\"examples\"]):\n",
    "            offset = 0.7\n",
    "            angle = np.pi/6 * (i - 1)\n",
    "            x = mod[\"pos\"][0] + offset * np.cos(angle)\n",
    "            y = mod[\"pos\"][1] + offset * np.sin(angle) - 0.8\n",
    "            ax.text(x, y, example, ha='center', va='center', fontsize=9, \n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.7))\n",
    "    \n",
    "    # Add applications around the edge\n",
    "    applications = [\n",
    "        \"Healthcare\", \"E-commerce\", \"Education\", \"Entertainment\",\n",
    "        \"Security\", \"Accessibility\", \"Research\", \"Creative Arts\"\n",
    "    ]\n",
    "    \n",
    "    for i, app in enumerate(applications):\n",
    "        angle = 2 * np.pi * i / len(applications)\n",
    "        x = 3.5 * np.cos(angle)\n",
    "        y = 3.5 * np.sin(angle)\n",
    "        ax.text(x, y, app, ha='center', va='center', fontsize=11, \n",
    "               style='italic', color='darkblue')\n",
    "    \n",
    "    ax.set_xlim(-4.5, 4.5)\n",
    "    ax.set_ylim(-4.5, 4.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('The Multimodal AI Ecosystem', fontsize=18, pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n🎉 You're now ready to build with multimodal AI!\")\n",
    "    print(\"\\nResources for continued learning:\")\n",
    "    print(\"• Hugging Face Model Hub: huggingface.co/models\")\n",
    "    print(\"• Papers With Code: paperswithcode.com\")\n",
    "    print(\"• Course Materials: huggingface.co/course\")\n",
    "    print(\"• Community: discuss.huggingface.co\")\n",
    "\n",
    "# Create final summary\n",
    "create_final_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}